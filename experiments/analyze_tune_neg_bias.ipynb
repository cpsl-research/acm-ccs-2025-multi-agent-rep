{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analysis_utils import load_metrics_from_subdir, wrap_metrics_hooks_to_df\n",
    "\n",
    "# set mpl font sizing\n",
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 28\n",
    "BIGGER_SIZE = 30\n",
    "\n",
    "plt.rc(\"font\", size=BIGGER_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=MEDIUM_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=BIGGER_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "font_family = \"serif\"\n",
    "\n",
    "# set seaborn styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "n_colors = 9\n",
    "cms_color = {\n",
    "    color: sns.color_palette(color, n_colors)\n",
    "    for color in [\"Blues\", \"Greens\", \"Reds\", \"Oranges\", \"Purples\", \"Greys\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loading of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_merged_df(mc_dir: str, hook_file_template: str = \"hooks_config.p\"):\n",
    "    # set output figure directory\n",
    "    fig_dir_out = os.path.join(\"figures\", mc_dir)\n",
    "    if os.path.exists(fig_dir_out):\n",
    "        shutil.rmtree(fig_dir_out)\n",
    "    os.makedirs(fig_dir_out)\n",
    "\n",
    "    # load in the metrics and hook configs\n",
    "    metrics_dir = os.path.join(\"metrics\", \"mc_tuning\", mc_dir)\n",
    "    metrics_trials, hooks_trials = load_metrics_from_subdir(\n",
    "        metrics_dir, hook_file_template\n",
    "    )\n",
    "    metrics_df, hooks_df = wrap_metrics_hooks_to_df(metrics_trials, hooks_trials)\n",
    "\n",
    "    # merge the two dictionaries\n",
    "    merged_df = pd.merge(\n",
    "        metrics_df, hooks_df, left_on=\"hook\", right_index=True, how=\"left\"\n",
    "    )\n",
    "    merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    merged_df.dropna(how=\"all\", inplace=True)\n",
    "    print(merged_df.shape)\n",
    "    merged_df.columns\n",
    "\n",
    "    return merged_df, fig_dir_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define plots of the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_percentiles(\n",
    "    df: pd.DataFrame,\n",
    "    ax,\n",
    "    cmap: list,\n",
    "    param_index: str,\n",
    "    outcome_index: str,\n",
    "    label: str,\n",
    "    show_percentiles: bool = True,\n",
    "    use_twin_ax: bool = False,\n",
    "):\n",
    "    \"\"\"Make marginal plot with percentiles\"\"\"\n",
    "    if use_twin_ax:\n",
    "        ax = ax.twinx()\n",
    "\n",
    "    # add the percentiles\n",
    "    if show_percentiles:\n",
    "        for ix, ci in enumerate(range(10, 100, 10)):\n",
    "            # add hues\n",
    "            sns.lineplot(\n",
    "                data=df,\n",
    "                x=param_index,\n",
    "                y=outcome_index,\n",
    "                errorbar=(\"ci\", ci),\n",
    "                ax=ax,\n",
    "                linestyle=\"\" if ci != 50 else \"-\",\n",
    "                hue=ci,\n",
    "                palette={ci: cmap[ix]},\n",
    "                legend=False,\n",
    "                linewidth=1,\n",
    "                label=\"\" if ci != 50 else label,\n",
    "            )\n",
    "\n",
    "    # always add the median line\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x=param_index,\n",
    "        y=outcome_index,\n",
    "        errorbar=(\"ci\", 50),\n",
    "        ax=ax,\n",
    "        linestyle=\"--\",\n",
    "        hue=50,\n",
    "        palette=[\"black\"] if show_percentiles else [cmap[4]],\n",
    "        legend=False,\n",
    "        linewidth=2,\n",
    "        alpha=0.5,\n",
    "        label=\"\" if show_percentiles else label,\n",
    "    )\n",
    "\n",
    "\n",
    "def marginal_plot_over_grouped_df(\n",
    "    ax,\n",
    "    df_grouped: pd.DataFrame,\n",
    "    loop_tuples: list,\n",
    "    param_index: str,\n",
    "    param_name: str,\n",
    "    show_legend: bool,\n",
    "    savefig_ext: str,\n",
    "    fig_save_dir: str,\n",
    "):\n",
    "    # plot confidence intervals for lines\n",
    "    for outcome_index, label, cmap_name in loop_tuples:\n",
    "        # make the percentile plots\n",
    "        make_plot_percentiles(\n",
    "            df=df_grouped,\n",
    "            ax=ax,\n",
    "            cmap=cms_color[cmap_name],\n",
    "            param_index=param_index,\n",
    "            outcome_index=outcome_index,\n",
    "            label=label,\n",
    "            show_percentiles=True,\n",
    "            use_twin_ax=False,\n",
    "        )\n",
    "\n",
    "    ax.set_title('MC Tuning \"{}\"'.format(param_name), family=font_family)\n",
    "    ax.set_xlabel(\"Parameter Value\", family=font_family)\n",
    "    ax.set_ylabel(\"Performance Metric\", family=font_family)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    if show_legend:\n",
    "        leg = ax.legend(\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, -0.19),\n",
    "            ncol=2,\n",
    "            fancybox=True,\n",
    "            shadow=True,\n",
    "            prop={\"family\": font_family},\n",
    "        )\n",
    "\n",
    "        # set the linewidth of each legend object\n",
    "        for legobj in leg.legend_handles:\n",
    "            legobj.set_linewidth(4.0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(\n",
    "            fig_save_dir, \"trust_tune_{}_{}.pdf\".format(param_index, savefig_ext)\n",
    "        )\n",
    "    )\n",
    "    plt.savefig(\n",
    "        os.path.join(\n",
    "            fig_save_dir, \"trust_tune_{}_{}.png\".format(param_index, savefig_ext)\n",
    "        )\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_marginal_parameter_tuning_plots(\n",
    "    merged_df: pd.DataFrame, fig_dir_out: str, param_index: str\n",
    "):\n",
    "    # define all the things to plot\n",
    "    param_name = (\" \".join(param_index.split(\"_\"))).title()\n",
    "    loop_tuples = [\n",
    "        (\"trusted_f1\", \"F1 Score on Trusted Tracks\", \"Reds\"),\n",
    "        (\"trusted_precision\", \"Precision on Trusted Tracks\", \"Purples\"),\n",
    "        (\"trusted_recall\", \"Recall on Trusted Tracks\", \"Greys\"),\n",
    "        (\"metric_agent\", \"Mean Agent Trust Metric\", \"Blues\"),\n",
    "        (\"metric_track\", \"Mean Track Trust Metric\", \"Greens\"),\n",
    "    ]\n",
    "\n",
    "    # take the median across frames at a given hook and trial\n",
    "    merged_df_grouped_median = merged_df.groupby(\n",
    "        by=[\"hook\", \"trial\", param_index]\n",
    "    ).median()\n",
    "    merged_df_grouped_twosigma_below = merged_df.groupby(\n",
    "        by=[\"hook\", \"trial\", param_index]\n",
    "    ).quantile(0.023)\n",
    "\n",
    "    # plot the median version\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 9))\n",
    "    marginal_plot_over_grouped_df(\n",
    "        ax=ax,\n",
    "        df_grouped=merged_df_grouped_median,\n",
    "        loop_tuples=loop_tuples,\n",
    "        param_index=param_index,\n",
    "        param_name=param_name,\n",
    "        show_legend=True,\n",
    "        savefig_ext=\"median\",\n",
    "        fig_save_dir=fig_dir_out,\n",
    "    )\n",
    "\n",
    "    # plot the (-2*sigma) version\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 9))\n",
    "    marginal_plot_over_grouped_df(\n",
    "        ax=ax,\n",
    "        df_grouped=merged_df_grouped_twosigma_below,\n",
    "        loop_tuples=loop_tuples,\n",
    "        param_index=param_index,\n",
    "        param_name=param_name,\n",
    "        show_legend=True,\n",
    "        savefig_ext=\"twosigmabelow\",\n",
    "        fig_save_dir=fig_dir_out,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot negativity bias marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_neg_bias, fig_neg_bias_out = load_merged_df(\"mc_trust_tune_neg_bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_index_neg_bias = \"agent_negativity_bias\"\n",
    "make_marginal_parameter_tuning_plots(\n",
    "    merged_df=merged_df_neg_bias,\n",
    "    fig_dir_out=fig_neg_bias_out,\n",
    "    param_index=param_index_neg_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Negativity Threshold Marginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_neg_bias_thresh, fig_neg_bias_thresh_out = load_merged_df(\n",
    "    \"mc_trust_tune_neg_bias_threshold\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_index_neg_bias = \"agent_negativity_bias\"\n",
    "param_index_neg_thresh = \"agent_negativity_threshold\"\n",
    "\n",
    "make_marginal_parameter_tuning_plots(\n",
    "    merged_df=merged_df_neg_bias_thresh,\n",
    "    fig_dir_out=fig_neg_bias_thresh_out,\n",
    "    param_index=param_index_neg_bias,\n",
    ")\n",
    "\n",
    "make_marginal_parameter_tuning_plots(\n",
    "    merged_df=merged_df_neg_bias_thresh,\n",
    "    fig_dir_out=fig_neg_bias_thresh_out,\n",
    "    param_index=param_index_neg_thresh,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Do a pair grid between the outcome metrics\n",
    "####################################################\n",
    "\n",
    "metrics_columns = [\n",
    "    \"trusted_f1\",\n",
    "    \"trusted_precision\",\n",
    "    \"trusted_recall\",\n",
    "    \"metric_agent\",\n",
    "    \"metric_track\",\n",
    "]\n",
    "g = sns.PairGrid(merged_df_grouped_median[metrics_columns], diag_sharey=False)\n",
    "g.map_upper(sns.histplot)\n",
    "g.map_lower(sns.kdeplot, fill=False)\n",
    "g.map_diag(sns.histplot, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Show the two-dimensional surface dependency\n",
    "# between negativity bias and threshold\n",
    "####################################################\n",
    "\n",
    "param_indices_2 = [\"agent_negativity_bias\", \"agent_negativity_threshold\"]\n",
    "param_name_2 = [\"Agent Negativity Bias\", \"Agent Negativity Threshold\"]\n",
    "loop_tuples_outcomes = [\n",
    "    (\"trusted_f1\", \"F1 Score on Trusted Tracks\", \"Reds\"),\n",
    "    (\"trusted_precision\", \"Precision on Trusted Tracks\", \"Purples\"),\n",
    "    (\"trusted_recall\", \"Recall on Trusted Tracks\", \"Greys\"),\n",
    "    (\"metric_agent\", \"Mean Agent Trust Metric\", \"Blues\"),\n",
    "    (\"metric_track\", \"Mean Track Trust Metric\", \"Greens\"),\n",
    "]\n",
    "idx_outcome = 0\n",
    "\n",
    "# bin the two parameters for robustness\n",
    "\n",
    "\n",
    "# take a median of the binned results\n",
    "\n",
    "\n",
    "# show the surface plot\n",
    "\n",
    "\n",
    "# set plot labels and things\n",
    "ax.set_title(\n",
    "    'Monte Carlo Tuning \"{}\" and \"{}\"'.format(*param_name_2), family=font_family\n",
    ")\n",
    "ax.set_xlabel(\"{} Parameter\".format(param_name_2[0]), family=font_family)\n",
    "ax.set_ylabel(\"{} Parameter\".format(param_name_2[1]), family=font_family)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(fig_out, \"trust_tune_{}_and_{}_surface.pdf\".format(*param_indices_2))\n",
    ")\n",
    "plt.savefig(\n",
    "    os.path.join(fig_out, \"trust_tune_{}_and_{}_surface.png\".format(*param_indices_2))\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
